{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Baseline model\n",
    "- The fist  linear models to solve the e.g. linear regression, logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/hung/.local/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in /home/hung/.local/lib/python3.7/site-packages (from seaborn) (1.18.0)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /home/hung/.local/lib/python3.7/site-packages (from seaborn) (0.25.3)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /home/hung/.local/lib/python3.7/site-packages (from seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/hung/.local/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/hung/.local/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hung/.local/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/hung/.local/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hung/.local/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.6.1->pandas>=0.15.2->seaborn) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /home/hung/.local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (44.0.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --user seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1714c9d6a2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas_summary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameSummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_summary import DataFrameSummary\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '_data/' \n",
    "df_train = pd.read_csv(data_dir + 'train1.csv')\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "# Simple handling of NAs\n",
    "def handle_na(df):\n",
    "    # Pre-process - drop attributes that has no valid values for most of the records\n",
    "    df = df.drop(['Alley', 'Fence','MiscFeature'], axis=1)\n",
    "\n",
    "    # Drop Pool information as well as not many of the sampled properties has a pool (7 out of 1460)\n",
    "    df = df.drop(['PoolArea', 'PoolQC'], axis=1)     \n",
    "    # Find categorical variables\n",
    "    types_df = pd.DataFrame(df.dtypes).reset_index()\n",
    "    cat_cols = types_df[types_df[0] == 'object']['index'].values\n",
    "    df[cat_cols] = df[cat_cols].fillna('Other')\n",
    "    return df.dropna()\n",
    "\n",
    "df_train_no_na = handle_na(df_train)\n",
    "types_df = pd.DataFrame(df_train_no_na.dtypes).reset_index()\n",
    "cat_cols = types_df[types_df[0] == 'object']['index'].values\n",
    "\n",
    "# Show skewness of continuous variables\n",
    "print(abs(df_train_no_na.skew()).sort_values(ascending=False))\n",
    "# Half of the numerical variables are highly skewed (skewness > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "summ = DataFrameSummary(df_train_no_na)\n",
    "summ.columns_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "def hot_encode_categorial_variables(df):\n",
    "    return pd.get_dummies(df)\n",
    "\n",
    "def predict_score(mymodel, xtrain, xtest, ytrain, ytest, verbose=True):\n",
    "    mymodel.fit(xtrain, ytrain)\n",
    "    #Predicting the prices\n",
    "    pred = mymodel.predict(xtest)\n",
    "    err_rms = np.sqrt(metrics.mean_squared_error(ytest, pred))/1000\n",
    "    return err_rms\n",
    "    \n",
    "def split_predict_score(mymodel, df):\n",
    "    X = df.drop(columns=target_col, axis=1)\n",
    "    y = df[target_col].values\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=1/4, random_state=0)\n",
    "    err = predict_score(mymodel, xtrain, xtest, ytrain, ytest)\n",
    "    print('Root Mean Squared Error (Scaled):', err)\n",
    "\n",
    "def xgb_predict(seeds,xtrain, xtest, ytrain, ytest):\n",
    "    rms = list()\n",
    "    for s in seeds:\n",
    "        p = xgb.XGBRegressor(s)\n",
    "        err = predict_score(p, xtrain, xtest, ytrain, ytest)\n",
    "        rms.append(err)\n",
    "    print(stats.describe(np.array(rms)))\n",
    "    \n",
    "def xgb_split_and_predict(seeds, df):\n",
    "    X = df.drop(columns=target_col, axis=1)\n",
    "    y = df[target_col].values\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=1/4, random_state=0)\n",
    "    xgb_predict(xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "\n",
    "print('Attempt 1: Linear regression (without categorical variables)')\n",
    "split_predict_score(LinearRegression(), df_train_no_na.drop(columns=cat_cols, axis=1))\n",
    "\n",
    "print('\\nAttempt 2: Linear regression (with categorical variables, one-hot encoded)')\n",
    "split_predict_score(LinearRegression(), hot_encode_categorial_variables(df_train_no_na))\n",
    "\n",
    "# Since linear regression performs better, let's try different linear models\n",
    "print('\\nAttempt 5: Ridge regression')\n",
    "split_predict_score(Ridge(), hot_encode_categorial_variables(df_train_no_na))\n",
    "\n",
    "print('\\nAttempt 6: Lasso regression')\n",
    "split_predict_score(Lasso(max_iter=100000), hot_encode_categorial_variables(df_train_no_na))\n",
    "\n",
    "# Ridge is a model designed to avoid over-fitting, it seems that we can use a linear small number of variables \n",
    "# where the target variable is linearly dependent on those variables and most variables are noise and does\n",
    "# not contribute to price?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
