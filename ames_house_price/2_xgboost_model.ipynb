{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model\n",
    "- Popular tree-boosting model (especially on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_summary import DataFrameSummary\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '_data/' \n",
    "df_train = pd.read_csv(data_dir + 'train1.csv')\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "# def preprocess(df):\n",
    "#     # Find categorical variables\n",
    "#     types_df = pd.DataFrame(df.dtypes).reset_index()\n",
    "#     cat_cols = types_df[types_df[0] == 'object']['index'].values\n",
    "#     # Assign NA as Other for these attributes, usually NA indicates Not Present\n",
    "#     #df[cat_cols] = df[cat_cols].fillna('Other')\n",
    "#     return df\n",
    "\n",
    "def drop_cols(df):\n",
    "    # Pre-process - drop attributes as most properties(>80%) do not have these features...\n",
    "    return df.drop(['Alley', 'Fence','MiscFeature','PoolArea', 'PoolQC'], axis=1)\n",
    "\n",
    "df_train = df_train.drop('Id', axis=1)\n",
    "df_train1 = drop_cols(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "def hot_encode_categorial_variables(df, required_cols=[]):\n",
    "    df = pd.get_dummies(df)\n",
    "    if len(required_cols) > 0:\n",
    "        for c in required_cols:\n",
    "            if c not in df.columns.values:\n",
    "                df[c] = 0\n",
    "    return df\n",
    "\n",
    "def predict_score(mymodel, xtrain, xtest, ytrain, ytest, verbose=True):\n",
    "    mymodel.fit(xtrain, ytrain)\n",
    "    #Predicting the prices\n",
    "    pred = mymodel.predict(xtest)\n",
    "    err_rms = np.sqrt(metrics.mean_squared_error(ytest, pred))/1000\n",
    "    return {'model':mymodel, 'err':err_rms}\n",
    "\n",
    "def xgb_predict(seeds,xtrain, xtest, ytrain, ytest):\n",
    "    rms = list()\n",
    "    model = None\n",
    "    for s in seeds:\n",
    "        p = xgb.XGBRegressor(s)\n",
    "        results = predict_score(p, xtrain, xtest, ytrain, ytest)\n",
    "        err = results['err']\n",
    "        rms.append(err)\n",
    "        if model is None or err < np.min(rms):\n",
    "            model = results['model']\n",
    "    print(stats.describe(np.array(rms)))\n",
    "    return model\n",
    "    \n",
    "def xgb_split_and_predict(seeds, df):\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col].values\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=1/4, random_state=0)\n",
    "    return xgb_predict(seeds, xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 6: XGBoost\n",
      "DescribeResult(nobs=10, minmax=(29.804433210279093, 30.26136839134928), mean=30.135484506927135, variance=0.014544722019494959, skewness=-2.279346177966918, kurtosis=4.15844086001757)\n",
      "\n",
      "Attempt 6: XGBoost with less attributes\n",
      "DescribeResult(nobs=10, minmax=(28.856005810332096, 29.278142335242553), mean=29.187269170831524, variance=0.023305546984801605, skewness=-1.253318016148924, kurtosis=0.07506844685155878)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "print('\\nAttempt 6: XGBoost')\n",
    "seeds=(np.random.random_sample((10,))*100).astype(int)\n",
    "xgboost1 = xgb_split_and_predict(seeds, hot_encode_categorial_variables(df_train))\n",
    "\n",
    "print('\\nAttempt 6: XGBoost with less attributes')\n",
    "xgboost2_train = hot_encode_categorial_variables(df_train1)\n",
    "xgboost2 = xgb_split_and_predict(seeds, xgboost2_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7fd38b7286a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hungap/programs/anaconda3/lib/python3.6/site-packages/xgboost/core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation estimator=BayesianRidge\n",
      "DescribeResult(nobs=10, minmax=(28.570264107191022, 81.25570685806929), mean=34.84732478961672, variance=266.48609667014375, skewness=2.655731304840336, kurtosis=5.0763580867134)\n",
      "Imputation estimator=DecisionTreeRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungap/programs/anaconda3/lib/python3.6/site-packages/sklearn/impute/_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=10, minmax=(29.73919697781643, 81.25570685806929), mean=35.15557875539179, variance=262.8448824298886, skewness=2.657880450276656, kurtosis=5.082990211712058)\n",
      "Imputation estimator=ExtraTreesRegressor\n",
      "DescribeResult(nobs=10, minmax=(29.59127321770937, 81.25570685806929), mean=35.326458700275296, variance=260.5470800467616, skewness=2.6644576624661225, kurtosis=5.10420866709884)\n",
      "Imputation estimator=KNeighborsRegressor\n",
      "DescribeResult(nobs=10, minmax=(29.41266468828967, 81.25570685806929), mean=34.899836409073295, variance=266.0081609339686, skewness=2.653575221789534, kurtosis=5.068879212402145)\n"
     ]
    }
   ],
   "source": [
    "# Let's see if we can improve this score by imputating missing NA value using multivariate imputation\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "estimators = [\n",
    "    BayesianRidge(),\n",
    "    DecisionTreeRegressor(max_features='sqrt', random_state=0),\n",
    "    ExtraTreesRegressor(n_estimators=10, random_state=0),\n",
    "    KNeighborsRegressor(n_neighbors=15)\n",
    "]\n",
    "\n",
    "def impute_and_predict(estimator, X, y):\n",
    "    print('Imputation estimator=' + estimator.__class__.__name__)\n",
    "    imp = IterativeImputer(random_state=0, estimator=estimator)\n",
    "    imp.fit(X)\n",
    "    X_transformed = imp.transform(X)\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X_transformed,y,test_size=1/4, random_state=0)\n",
    "    xgb_predict(seeds,xtrain, xtest, ytrain, ytest)\n",
    "    return imp\n",
    "\n",
    "X = hot_encode_categorial_variables(pd.get_dummies(df_train.drop(target_col, axis=1)))\n",
    "y = df_train[target_col]\n",
    "for e in estimators:\n",
    "    impute_and_predict(e,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use second XGBoost model to submit first predictions\n",
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "test_train =  hot_encode_categorial_variables(drop_cols(preprocess(test)), xgboost2_train.columns)\n",
    "test_train = test_train.drop(['Id','SalePrice'], axis=1)\n",
    "testy = xgboost2.predict(test_train[xgboost2_train.drop('SalePrice', axis=1).columns])\n",
    "test['SalePrice'] = testy\n",
    "test[['Id', 'SalePrice']].to_csv(data_dir + 'prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungap/programs/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation estimator=BayesianRidge\n",
      "DescribeResult(nobs=10, minmax=(26.166011981512124, 26.73323052804383), mean=26.448759425151728, variance=0.02223945921456005, skewness=0.23399876642647374, kurtosis=0.5241395830990787)\n",
      "Imputation estimator=DecisionTreeRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungap/programs/anaconda3/lib/python3.6/site-packages/sklearn/impute/_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=10, minmax=(26.166011981512124, 26.73323052804383), mean=26.448759425151728, variance=0.02223945921456005, skewness=0.23399876642647374, kurtosis=0.5241395830990787)\n",
      "Imputation estimator=ExtraTreesRegressor\n"
     ]
    }
   ],
   "source": [
    "# Impute with test set concatenated with training set\n",
    "X_all = X.append(test_train)\n",
    "for e in estimators:\n",
    "    print('Imputation estimator=' + e.__class__.__name__)\n",
    "    imp = IterativeImputer(random_state=0, estimator=e)\n",
    "    imp.fit(X_all)\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(imp.transform(X),y,test_size=1/4, random_state=0)\n",
    "    xgb_predict(seeds,xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 271 features per sample, expected 288",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-6331e945cd4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate another submission wtih amputation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtestx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboost2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hungap/programs/anaconda3/lib/python3.6/site-packages/sklearn/impute/_iterative.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mX_trans_indicator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_missing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_imputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_missing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hungap/programs/anaconda3/lib/python3.6/site-packages/sklearn/impute/_iterative.py\u001b[0m in \u001b[0;36m_initial_imputation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mX_filled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_imputer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mX_filled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_imputer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         valid_mask = np.flatnonzero(np.logical_not(\n",
      "\u001b[0;32m/home/hungap/programs/anaconda3/lib/python3.6/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             raise ValueError(\"X has %d features per sample, expected %d\"\n\u001b[0;32m--> 384\u001b[0;31m                              % (X.shape[1], self.statistics_.shape[0]))\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_indicator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 271 features per sample, expected 288"
     ]
    }
   ],
   "source": [
    "# Generate another submission wtih amputation\n",
    "testx = pd.get_dummies(test.drop(target_col, axis=1))\n",
    "imp_model = xgboost2.predict(imp.transform(testx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
